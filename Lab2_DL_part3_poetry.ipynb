{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zinkin-s/homework/blob/main/Lab2_DL_part3_poetry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qEVazsIsPTvF"
      },
      "source": [
        "## Lab 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQbW2NKdPTvH"
      },
      "source": [
        "### Part 3. Poetry generation\n",
        "\n",
        "Let's try to generate some poetry using RNNs.\n",
        "\n",
        "You have several choices here:\n",
        "\n",
        "* The Shakespeare sonnets, file `sonnets.txt` available in the notebook directory.\n",
        "\n",
        "* Роман в стихах \"Евгений Онегин\" Александра Сергеевича Пушкина. В предобработанном виде доступен по [ссылке](https://github.com/attatrol/data_sources/blob/master/onegin.txt).\n",
        "\n",
        "* Some other text source, if it will be approved by the course staff.\n",
        "\n",
        "Text generation can be designed in several steps:\n",
        "    \n",
        "1. Data loading.\n",
        "2. Dictionary generation.\n",
        "3. Data preprocessing.\n",
        "4. Model (neural network) training.\n",
        "5. Text generation (model evaluation).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1akxjlwAPTvI"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JI6kK1rjPTvI"
      },
      "source": [
        "### Data loading: Shakespeare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvg1Nrv_PTvI"
      },
      "source": [
        "Shakespeare sonnets are awailable at this [link](http://www.gutenberg.org/ebooks/1041?msg=welcome_stranger). In addition, they are stored in the same directory as this notebook (`sonnetes.txt`). Simple preprocessing is already done for you in the next cell: all technical info is dropped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tutgD_FPPTvJ",
        "outputId": "ef8efb19-93e4-421c-b1ca-7b116f1e8ef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-23 14:51:57--  https://raw.githubusercontent.com/girafe-ai/ml-course/22f_basic/homeworks/lab02_deep_learning/sonnets.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 119748 (117K) [text/plain]\n",
            "Saving to: ‘sonnets.txt’\n",
            "\n",
            "sonnets.txt         100%[===================>] 116.94K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2025-04-23 14:51:57 (47.6 MB/s) - ‘sonnets.txt’ saved [119748/119748]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if not os.path.exists('sonnets.txt'):\n",
        "    !wget https://raw.githubusercontent.com/girafe-ai/ml-course/22f_basic/homeworks/lab02_deep_learning/sonnets.txt\n",
        "\n",
        "with open('sonnets.txt', 'r') as iofile:\n",
        "    text = iofile.readlines()\n",
        "\n",
        "TEXT_START = 45\n",
        "TEXT_END = -368\n",
        "text = text[TEXT_START : TEXT_END]\n",
        "assert len(text) == 2616"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daql627xPTvJ"
      },
      "source": [
        "In opposite to the in-class practice, this time we want to predict complex text. Let's reduce the complexity of the task and lowercase all the symbols.\n",
        "\n",
        "Now variable `text` is a list of strings. Join all the strings into one and lowercase it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFFUb_tNPTvJ",
        "outputId": "51684bd3-7f8a-4547-c69b-23f7d275798b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK!\n"
          ]
        }
      ],
      "source": [
        "# Join all the strings into one and lowercase it\n",
        "# Put result into variable text.\n",
        "\n",
        "# Your great code here\n",
        "text = ''.join(text).lower()\n",
        "assert len(text) == 100225, 'Are you sure you have concatenated all the strings?'\n",
        "assert not any([x in set(text) for x in string.ascii_uppercase]), 'Uppercase letters are present'\n",
        "print('OK!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epOFih0DPTvJ"
      },
      "source": [
        "### Data loading: \"Евгений Онегин\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BiZJlAW7PTvJ",
        "outputId": "e51d1a51-63b6-4a85-d777-dbacc05ac568"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-23 15:25:59--  https://raw.githubusercontent.com/attatrol/data_sources/master/onegin.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 262521 (256K) [text/plain]\n",
            "Saving to: ‘onegin.txt’\n",
            "\n",
            "onegin.txt          100%[===================>] 256.37K  --.-KB/s    in 0.004s  \n",
            "\n",
            "2025-04-23 15:25:59 (65.2 MB/s) - ‘onegin.txt’ saved [262521/262521]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/attatrol/data_sources/master/onegin.txt\n",
        "\n",
        "with open('onegin.txt', 'r') as iofile:\n",
        "    text = iofile.readlines()\n",
        "\n",
        "text = [x.replace('\\t\\t', '') for x in text]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDVrp2rcPTvK"
      },
      "source": [
        "In opposite to the in-class practice, this time we want to predict complex text. Let's reduce the complexity of the task and lowercase all the symbols.\n",
        "\n",
        "Now variable `text` is a list of strings. Join all the strings into one and lowercase it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PQv6csPgPTvK"
      },
      "outputs": [],
      "source": [
        "# Join all the strings into one and lowercase it\n",
        "# Put result into variable text.\n",
        "\n",
        "# Your great code here\n",
        "out = ''.join(text).lower()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj__DSMDPTvK"
      },
      "source": [
        "Put all the characters, that you've seen in the text, into variable `tokens`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TdlraXNuPTvK"
      },
      "outputs": [],
      "source": [
        "tokens = sorted(set(out))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwH3ukchPTvK"
      },
      "source": [
        "Create dictionary `token_to_idx = {<char>: <index>}` and dictionary `idx_to_token = {<index>: <char>}`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "PbfyzcQbPTvL"
      },
      "outputs": [],
      "source": [
        "# dict <index>:<char>\n",
        "# Your great code here\n",
        "\n",
        "# dict <char>:<index>\n",
        "# Your great code here\n",
        "token_to_idx = {token: idx for idx, token in enumerate(tokens)}\n",
        "idx_to_token = {idx: char for char, idx in token_to_idx.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjQhBn2XPTvL"
      },
      "source": [
        "*Comment: in this task we have only 38 different tokens, so let's use one-hot encoding.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDd9SytYPTvL"
      },
      "source": [
        "### Building the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXegH2-GPTvL"
      },
      "source": [
        "Now we want to build and train recurrent neural net which would be able to something similar to Shakespeare's poetry.\n",
        "\n",
        "Let's use vanilla RNN, similar to the one created during the lesson."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZ6nIn3sPTvL",
        "outputId": "d308a673-5ca8-4aa5-f842-9cd906437b68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated chunk: народных слез, рукоплесканий\n",
            "с младой семеновой делил;\n",
            "там наш катенин воскресил\n",
            "корнеля гений велича\n",
            "Epoch: 100 / 10000. Average Loss: 4.1239\n",
            "Epoch: 200 / 10000. Average Loss: 3.3338\n",
            "Epoch: 300 / 10000. Average Loss: 3.1957\n",
            "Epoch: 400 / 10000. Average Loss: 3.1411\n",
            "Epoch: 500 / 10000. Average Loss: 3.0584\n",
            "Epoch: 600 / 10000. Average Loss: 3.0171\n",
            "Epoch: 700 / 10000. Average Loss: 2.9209\n",
            "Epoch: 800 / 10000. Average Loss: 2.8370\n",
            "Epoch: 900 / 10000. Average Loss: 2.7979\n",
            "Epoch: 1000 / 10000. Average Loss: 2.7481\n",
            "Epoch: 1100 / 10000. Average Loss: 2.6993\n",
            "Epoch: 1200 / 10000. Average Loss: 2.6412\n",
            "Epoch: 1300 / 10000. Average Loss: 2.6193\n",
            "Epoch: 1400 / 10000. Average Loss: 2.5765\n",
            "Epoch: 1500 / 10000. Average Loss: 2.5550\n",
            "Epoch: 1600 / 10000. Average Loss: 2.5077\n",
            "Epoch: 1700 / 10000. Average Loss: 2.5020\n",
            "Epoch: 1800 / 10000. Average Loss: 2.4622\n",
            "Epoch: 1900 / 10000. Average Loss: 2.4421\n",
            "Epoch: 2000 / 10000. Average Loss: 2.4183\n",
            "Epoch: 2100 / 10000. Average Loss: 2.3976\n",
            "Epoch: 2200 / 10000. Average Loss: 2.3851\n",
            "Epoch: 2300 / 10000. Average Loss: 2.3595\n",
            "Epoch: 2400 / 10000. Average Loss: 2.3108\n",
            "Epoch: 2500 / 10000. Average Loss: 2.3208\n",
            "Epoch: 2600 / 10000. Average Loss: 2.2835\n",
            "Epoch: 2700 / 10000. Average Loss: 2.2742\n",
            "Epoch: 2800 / 10000. Average Loss: 2.2546\n",
            "Epoch: 2900 / 10000. Average Loss: 2.2174\n",
            "Epoch: 3000 / 10000. Average Loss: 2.2091\n",
            "Epoch: 3100 / 10000. Average Loss: 2.2007\n",
            "Epoch: 3200 / 10000. Average Loss: 2.1845\n",
            "Epoch: 3300 / 10000. Average Loss: 2.1428\n",
            "Epoch: 3400 / 10000. Average Loss: 2.1517\n",
            "Epoch: 3500 / 10000. Average Loss: 2.1054\n",
            "Epoch: 3600 / 10000. Average Loss: 2.1016\n",
            "Epoch: 3700 / 10000. Average Loss: 2.1163\n",
            "Epoch: 3800 / 10000. Average Loss: 2.1204\n",
            "Epoch: 3900 / 10000. Average Loss: 2.0971\n",
            "Epoch: 4000 / 10000. Average Loss: 2.0774\n",
            "Epoch: 4100 / 10000. Average Loss: 2.0647\n",
            "Epoch: 4200 / 10000. Average Loss: 2.0322\n",
            "Epoch: 4300 / 10000. Average Loss: 2.0442\n",
            "Epoch: 4400 / 10000. Average Loss: 2.0127\n",
            "Epoch: 4500 / 10000. Average Loss: 1.9958\n",
            "Epoch: 4600 / 10000. Average Loss: 1.9741\n",
            "Epoch: 4700 / 10000. Average Loss: 1.9840\n",
            "Epoch: 4800 / 10000. Average Loss: 1.9499\n",
            "Epoch: 4900 / 10000. Average Loss: 1.9572\n",
            "Epoch: 5000 / 10000. Average Loss: 1.9435\n",
            "Epoch: 5100 / 10000. Average Loss: 1.9752\n",
            "Epoch: 5200 / 10000. Average Loss: 1.9419\n",
            "Epoch: 5300 / 10000. Average Loss: 1.9210\n",
            "Epoch: 5400 / 10000. Average Loss: 1.8942\n",
            "Epoch: 5500 / 10000. Average Loss: 1.9034\n",
            "Epoch: 5600 / 10000. Average Loss: 1.9177\n",
            "Epoch: 5700 / 10000. Average Loss: 1.9063\n",
            "Epoch: 5800 / 10000. Average Loss: 1.8829\n",
            "Epoch: 5900 / 10000. Average Loss: 1.8677\n",
            "Epoch: 6000 / 10000. Average Loss: 1.8804\n",
            "Epoch: 6100 / 10000. Average Loss: 1.8442\n",
            "Epoch: 6200 / 10000. Average Loss: 1.8528\n",
            "Epoch: 6300 / 10000. Average Loss: 1.8371\n",
            "Epoch: 6400 / 10000. Average Loss: 1.8402\n",
            "Epoch: 6500 / 10000. Average Loss: 1.8346\n",
            "Epoch: 6600 / 10000. Average Loss: 1.8243\n",
            "Epoch: 6700 / 10000. Average Loss: 1.8063\n",
            "Epoch: 6800 / 10000. Average Loss: 1.7963\n",
            "Epoch: 6900 / 10000. Average Loss: 1.7938\n",
            "Epoch: 7000 / 10000. Average Loss: 1.7619\n",
            "Epoch: 7100 / 10000. Average Loss: 1.7418\n",
            "Epoch: 7200 / 10000. Average Loss: 1.7619\n",
            "Epoch: 7300 / 10000. Average Loss: 1.7776\n",
            "Epoch: 7400 / 10000. Average Loss: 1.7962\n",
            "Epoch: 7500 / 10000. Average Loss: 1.7598\n",
            "Epoch: 7600 / 10000. Average Loss: 1.7650\n",
            "Epoch: 7700 / 10000. Average Loss: 1.7229\n",
            "Epoch: 7800 / 10000. Average Loss: 1.7078\n",
            "Epoch: 7900 / 10000. Average Loss: 1.7133\n",
            "Epoch: 8000 / 10000. Average Loss: 1.7424\n",
            "Epoch: 8100 / 10000. Average Loss: 1.7108\n",
            "Epoch: 8200 / 10000. Average Loss: 1.7224\n",
            "Epoch: 8300 / 10000. Average Loss: 1.7025\n",
            "Epoch: 8400 / 10000. Average Loss: 1.6786\n",
            "Epoch: 8500 / 10000. Average Loss: 1.6804\n",
            "Epoch: 8600 / 10000. Average Loss: 1.6984\n",
            "Epoch: 8700 / 10000. Average Loss: 1.6878\n",
            "Epoch: 8800 / 10000. Average Loss: 1.6758\n",
            "Epoch: 8900 / 10000. Average Loss: 1.6509\n",
            "Epoch: 9000 / 10000. Average Loss: 1.6917\n",
            "Epoch: 9100 / 10000. Average Loss: 1.6490\n",
            "Epoch: 9200 / 10000. Average Loss: 1.6705\n",
            "Epoch: 9300 / 10000. Average Loss: 1.6699\n",
            "Epoch: 9400 / 10000. Average Loss: 1.6037\n",
            "Epoch: 9500 / 10000. Average Loss: 1.6739\n",
            "Epoch: 9600 / 10000. Average Loss: 1.6686\n",
            "Epoch: 9700 / 10000. Average Loss: 1.6595\n",
            "Epoch: 9800 / 10000. Average Loss: 1.6345\n",
            "Epoch: 9900 / 10000. Average Loss: 1.6191\n",
            "Epoch: 10000 / 10000. Average Loss: 1.6200\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "\n",
        "num_tokens = len(token_to_idx)\n",
        "seq_length = 100\n",
        "\n",
        "def chunk_to_tensor(chunk):\n",
        "    indices = [token_to_idx[letter] for letter in chunk]\n",
        "    tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(1)\n",
        "    one_hot_tensor = torch.nn.functional.one_hot(tensor, num_classes=num_tokens).float()\n",
        "    return one_hot_tensor\n",
        "\n",
        "def generate_chunk():\n",
        "    start_index = random.randint(0, len(text) - seq_length - 1)\n",
        "    end_index = start_index + seq_length + 1\n",
        "    return out[start_index:end_index]\n",
        "\n",
        "def training_set():\n",
        "    chunk = generate_chunk()\n",
        "    inp = chunk_to_tensor(chunk[:-1])\n",
        "    target = torch.tensor([token_to_idx[letter] for letter in chunk[1:]], dtype=torch.long).unsqueeze(1)\n",
        "    return inp, target\n",
        "\n",
        "class CharRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(CharRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        combined = torch.cat((input, hidden), dim=1)\n",
        "        hidden = self.i2h(combined)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size=1):\n",
        "        return torch.zeros(batch_size, self.hidden_size, device=device)\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "n_hidden = 128\n",
        "rnn = CharRNN(num_tokens, n_hidden, num_tokens)\n",
        "rnn.to(device)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "learning_rate = 1e-3\n",
        "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
        "\n",
        "def train(inp, target):\n",
        "    rnn.zero_grad()\n",
        "    hidden = rnn.init_hidden().to(device)\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(inp.size()[0]):\n",
        "        output, hidden = rnn(inp[i].to(device), hidden)\n",
        "        loss += criterion(output, target[i].to(device))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return output, loss.item() / seq_length\n",
        "\n",
        "n_epochs = 10000\n",
        "current_loss = 0\n",
        "all_losses = []\n",
        "\n",
        "# Проверка функции generate_chunk\n",
        "chunk = generate_chunk()\n",
        "print(f\"Generated chunk: {chunk}\")\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    inp, target = training_set()\n",
        "    output, loss = train(inp, target)\n",
        "    current_loss += loss\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch: {epoch} / {n_epochs}. Average Loss: {current_loss / 100:.4f}')\n",
        "        all_losses.append(current_loss / 100)\n",
        "        current_loss = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN0tI6fRPTvL"
      },
      "source": [
        "Plot the loss function (axis X: number of epochs, axis Y: loss function)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "Pra5FOy8PTvL",
        "outputId": "7fce6ad8-a516-4ed3-c4d9-81dde8d5d2c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7e8c4c794250>]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Your plot code here\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(all_losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zhqglWkPTvL"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "def generate_sample(char_rnn, seed_phrase='wh', max_length=seq_length, temperature=1.0):\n",
        "    '''\n",
        "    ### Disclaimer: this is an example function for text generation.\n",
        "    ### You can either adapt it in your code or create your own function\n",
        "\n",
        "    The function generates text given a phrase of length at least SEQ_LENGTH.\n",
        "    :param seed_phrase: prefix characters. The RNN is asked to continue the phrase\n",
        "    :param max_length: maximum output length, including seed_phrase\n",
        "    :param temperature: coefficient for sampling.  higher temperature produces more chaotic outputs,\n",
        "        smaller temperature converges to the single most likely output.\n",
        "\n",
        "    Be careful with the model output. This model waits logits (not probabilities/log-probabilities)\n",
        "    of the next symbol.\n",
        "    '''\n",
        "\n",
        "    #x_sequence = [token_to_idx[token] for token in seed_phrase]\n",
        "    x_sequence = chunk_to_tensor(seed_phrase)\n",
        "    #hid_state = char_rnn.initial_state(batch_size=1)\n",
        "\n",
        "    #feed the seed phrase, if any\n",
        "    for i in range(len(seed_phrase) - 1):\n",
        "        print(x_sequence[:, -1].shape)\n",
        "        out, hid_state = char_rnn(x_sequence[i].unsqueeze(0), None if i == 0 else hid_state)\n",
        "\n",
        "    #start generating\n",
        "    for i in range(max_length - len(seed_phrase)):\n",
        "        print(x_sequence.shape, x_sequence)\n",
        "        out, hid_state = char_rnn(x_sequence[i].unsqueeze(0))\n",
        "        # Be really careful here with the model output\n",
        "        p_next = F.softmax(out / temperature, dim=-1).data.numpy()[0]\n",
        "\n",
        "        # sample next token and push it back into x_sequence\n",
        "        print(p_next.shape, len(tokens))\n",
        "        next_ix = np.random.choice(len(tokens), p=p_next)\n",
        "        next_ix = torch.tensor([[next_ix]], dtype=torch.int64)\n",
        "        print(x_sequence.shape, next_ix.shape)\n",
        "        x_sequence = torch.cat([x_sequence, next_ix], dim=1)\n",
        "\n",
        "    return ''.join([tokens[ix] for ix in x_sequence.data.numpy()[0]])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, temperature, seed_phrase, max_length=500):\n",
        "    model.eval()\n",
        "    hidden = model.init_hidden()\n",
        "    result = seed_phrase\n",
        "    input = chunk_to_tensor(seed_phrase).to(device)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        output, hidden = model(input[-1], hidden)\n",
        "        output_dist = output.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "\n",
        "        predicted_char = idx_to_token[top_i.item()]\n",
        "        result += predicted_char\n",
        "        input = chunk_to_tensor(result[-len(seed_phrase):]).to(device)\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "0ARr5zCLbc2A"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KpRp0faRPTvL",
        "outputId": "eab3234c-067d-455b-faff-b7cde443b145"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "мой дядя самых честных правил в зовет,\n",
            "и дыла де подлуго просбудеть воним,\n",
            "чтобых раскузарнечной промужден себязья тери\n",
            "\n",
            "\n",
            "м роско пось я зорат\n",
            "и отчестын симер добыет, прознон коверизать прешук,\n",
            "сек ой стристо натавись,\n",
            "натов содений,\n",
            "чео завости бружеть ерышолять,\n",
            "полать перкек слетой,\n",
            "треско казнак езен взонеть\n",
            "и сяетя завалья храний сте сердата поспризать? стею зервенне порным, заворонью\n",
            "себыть ятереник ка поревин,\n",
            "что вопрани! когса сахор улчтять, шабогорок юбять я граждын\n",
            "\n",
            "\n",
            "огоной нежеж попот; там,\n",
            "чен ко промут белья \n"
          ]
        }
      ],
      "source": [
        "# An example of generated text.\n",
        "print(generate_text(rnn, seed_phrase='мой дядя самых честных правил', max_length=500,temperature=0.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jumNBFKhPTvM"
      },
      "source": [
        "### More poetic model\n",
        "\n",
        "Let's use LSTM instead of vanilla RNN and compare the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPQZ1wMTPTvM"
      },
      "source": [
        "Plot the loss function of the number of epochs. Does the final loss become better?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "hcZ8AECMPTvM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53e8a3dd-33d1-4d32-c68c-b569672ae5ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated chunk: перва madame за ним ходила,\n",
            "потом monsieur ее сменил;\n",
            "ребенок был резов, но мил.\n",
            "monsieur l’abbe€, фр\n",
            "Epoch: 100 / 10000. Average Loss: 3.4157\n",
            "Epoch: 200 / 10000. Average Loss: 3.0284\n",
            "Epoch: 300 / 10000. Average Loss: 2.7820\n",
            "Epoch: 400 / 10000. Average Loss: 2.7114\n",
            "Epoch: 500 / 10000. Average Loss: 2.6092\n",
            "Epoch: 600 / 10000. Average Loss: 2.5400\n",
            "Epoch: 700 / 10000. Average Loss: 2.4842\n",
            "Epoch: 800 / 10000. Average Loss: 2.4239\n",
            "Epoch: 900 / 10000. Average Loss: 2.3650\n",
            "Epoch: 1000 / 10000. Average Loss: 2.3225\n",
            "Epoch: 1100 / 10000. Average Loss: 2.2897\n",
            "Epoch: 1200 / 10000. Average Loss: 2.2366\n",
            "Epoch: 1300 / 10000. Average Loss: 2.1781\n",
            "Epoch: 1400 / 10000. Average Loss: 2.1524\n",
            "Epoch: 1500 / 10000. Average Loss: 2.1472\n",
            "Epoch: 1600 / 10000. Average Loss: 2.0894\n",
            "Epoch: 1700 / 10000. Average Loss: 2.0456\n",
            "Epoch: 1800 / 10000. Average Loss: 2.0095\n",
            "Epoch: 1900 / 10000. Average Loss: 1.9667\n",
            "Epoch: 2000 / 10000. Average Loss: 1.9374\n",
            "Epoch: 2100 / 10000. Average Loss: 1.8803\n",
            "Epoch: 2200 / 10000. Average Loss: 1.8590\n",
            "Epoch: 2300 / 10000. Average Loss: 1.7569\n",
            "Epoch: 2400 / 10000. Average Loss: 1.7481\n",
            "Epoch: 2500 / 10000. Average Loss: 1.6775\n",
            "Epoch: 2600 / 10000. Average Loss: 1.6493\n",
            "Epoch: 2700 / 10000. Average Loss: 1.5918\n",
            "Epoch: 2800 / 10000. Average Loss: 1.5344\n",
            "Epoch: 2900 / 10000. Average Loss: 1.5274\n",
            "Epoch: 3000 / 10000. Average Loss: 1.4233\n",
            "Epoch: 3100 / 10000. Average Loss: 1.4067\n",
            "Epoch: 3200 / 10000. Average Loss: 1.3755\n",
            "Epoch: 3300 / 10000. Average Loss: 1.3335\n",
            "Epoch: 3400 / 10000. Average Loss: 1.3045\n",
            "Epoch: 3500 / 10000. Average Loss: 1.2540\n",
            "Epoch: 3600 / 10000. Average Loss: 1.1987\n",
            "Epoch: 3700 / 10000. Average Loss: 1.1620\n",
            "Epoch: 3800 / 10000. Average Loss: 1.1523\n",
            "Epoch: 3900 / 10000. Average Loss: 1.1766\n",
            "Epoch: 4000 / 10000. Average Loss: 1.1074\n",
            "Epoch: 4100 / 10000. Average Loss: 1.0820\n",
            "Epoch: 4200 / 10000. Average Loss: 1.0176\n",
            "Epoch: 4300 / 10000. Average Loss: 1.0200\n",
            "Epoch: 4400 / 10000. Average Loss: 1.0248\n",
            "Epoch: 4500 / 10000. Average Loss: 0.9404\n",
            "Epoch: 4600 / 10000. Average Loss: 0.9074\n",
            "Epoch: 4700 / 10000. Average Loss: 0.9146\n",
            "Epoch: 4800 / 10000. Average Loss: 0.9653\n",
            "Epoch: 4900 / 10000. Average Loss: 0.9080\n",
            "Epoch: 5000 / 10000. Average Loss: 0.8396\n",
            "Epoch: 5100 / 10000. Average Loss: 0.8865\n",
            "Epoch: 5200 / 10000. Average Loss: 0.8145\n",
            "Epoch: 5300 / 10000. Average Loss: 0.8730\n",
            "Epoch: 5400 / 10000. Average Loss: 0.8267\n",
            "Epoch: 5500 / 10000. Average Loss: 0.8126\n",
            "Epoch: 5600 / 10000. Average Loss: 0.8169\n",
            "Epoch: 5700 / 10000. Average Loss: 0.7874\n",
            "Epoch: 5800 / 10000. Average Loss: 0.7907\n",
            "Epoch: 5900 / 10000. Average Loss: 0.8069\n",
            "Epoch: 6000 / 10000. Average Loss: 0.7110\n",
            "Epoch: 6100 / 10000. Average Loss: 0.7262\n",
            "Epoch: 6200 / 10000. Average Loss: 0.6689\n",
            "Epoch: 6300 / 10000. Average Loss: 0.6935\n",
            "Epoch: 6400 / 10000. Average Loss: 0.6482\n",
            "Epoch: 6500 / 10000. Average Loss: 0.6965\n",
            "Epoch: 6600 / 10000. Average Loss: 0.7080\n",
            "Epoch: 6700 / 10000. Average Loss: 0.6813\n",
            "Epoch: 6800 / 10000. Average Loss: 0.6716\n",
            "Epoch: 6900 / 10000. Average Loss: 0.6618\n",
            "Epoch: 7000 / 10000. Average Loss: 0.6409\n",
            "Epoch: 7100 / 10000. Average Loss: 0.6452\n",
            "Epoch: 7200 / 10000. Average Loss: 0.6464\n",
            "Epoch: 7300 / 10000. Average Loss: 0.6305\n",
            "Epoch: 7400 / 10000. Average Loss: 0.6016\n",
            "Epoch: 7500 / 10000. Average Loss: 0.5927\n",
            "Epoch: 7600 / 10000. Average Loss: 0.5841\n",
            "Epoch: 7700 / 10000. Average Loss: 0.5798\n",
            "Epoch: 7800 / 10000. Average Loss: 0.5817\n",
            "Epoch: 7900 / 10000. Average Loss: 0.5404\n",
            "Epoch: 8000 / 10000. Average Loss: 0.5788\n",
            "Epoch: 8100 / 10000. Average Loss: 0.5731\n",
            "Epoch: 8200 / 10000. Average Loss: 0.5712\n",
            "Epoch: 8300 / 10000. Average Loss: 0.5374\n",
            "Epoch: 8400 / 10000. Average Loss: 0.5579\n",
            "Epoch: 8500 / 10000. Average Loss: 0.5600\n",
            "Epoch: 8600 / 10000. Average Loss: 0.5998\n",
            "Epoch: 8700 / 10000. Average Loss: 0.5733\n",
            "Epoch: 8800 / 10000. Average Loss: 0.5327\n",
            "Epoch: 8900 / 10000. Average Loss: 0.5297\n",
            "Epoch: 9000 / 10000. Average Loss: 0.5156\n",
            "Epoch: 9100 / 10000. Average Loss: 0.5043\n",
            "Epoch: 9200 / 10000. Average Loss: 0.5015\n",
            "Epoch: 9300 / 10000. Average Loss: 0.4873\n",
            "Epoch: 9400 / 10000. Average Loss: 0.4956\n",
            "Epoch: 9500 / 10000. Average Loss: 0.5147\n",
            "Epoch: 9600 / 10000. Average Loss: 0.5291\n",
            "Epoch: 9700 / 10000. Average Loss: 0.5611\n",
            "Epoch: 9800 / 10000. Average Loss: 0.4819\n",
            "Epoch: 9900 / 10000. Average Loss: 0.4475\n",
            "Epoch: 10000 / 10000. Average Loss: 0.4613\n"
          ]
        }
      ],
      "source": [
        "# Your beautiful code here\n",
        "class LSTMCell(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(LSTMCell, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.i2i = nn.Linear(input_size, hidden_size)\n",
        "        self.h2i = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.i2f = nn.Linear(input_size, hidden_size)\n",
        "        self.h2f = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.i2c = nn.Linear(input_size, hidden_size)\n",
        "        self.h2c = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.i2o = nn.Linear(input_size, hidden_size)\n",
        "        self.h2o = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        hx, cx = hidden\n",
        "        ingate = self.sigmoid(self.i2i(input) + self.h2i(hx))\n",
        "        forgetgate = self.sigmoid(self.i2f(input) + self.h2f(hx))\n",
        "        cellgate = self.tanh(self.i2c(input) + self.h2c(hx))\n",
        "        outgate = self.sigmoid(self.i2o(input) + self.h2o(hx))\n",
        "\n",
        "        cy = (forgetgate * cx) + (ingate * cellgate)\n",
        "        hy = outgate * self.tanh(cy)\n",
        "        output = self.softmax(self.h2o(hy))\n",
        "        return output, (hy, cy)\n",
        "\n",
        "    def init_hidden(self, batch_size=1):\n",
        "        return (torch.zeros(batch_size, self.hidden_size, device=device),\n",
        "                torch.zeros(batch_size, self.hidden_size, device=device))\n",
        "\n",
        "n_hidden = 128\n",
        "lstm = LSTMCell(num_tokens, n_hidden)\n",
        "lstm.to(device)\n",
        "\n",
        "criterion = nn.NLLLoss()\n",
        "learning_rate = 0.005\n",
        "optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
        "\n",
        "def lstm_train(inp, target):\n",
        "    lstm.zero_grad()\n",
        "    hidden = lstm.init_hidden()\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(inp.size()[0]):\n",
        "        output, hidden = lstm(inp[i].to(device), hidden)\n",
        "        loss += criterion(output, target[i].to(device))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return output, loss.item() / seq_length\n",
        "\n",
        "n_epochs = 10000\n",
        "current_loss = 0\n",
        "all_losses = []\n",
        "\n",
        "# Проверка функции generate_chunk\n",
        "chunk = generate_chunk()\n",
        "print(f\"Generated chunk: {chunk}\")\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    inp, target = training_set()\n",
        "    output, loss = lstm_train(inp, target)\n",
        "    current_loss += loss\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch: {epoch} / {n_epochs}. Average Loss: {current_loss / 100:.4f}')\n",
        "        all_losses.append(current_loss / 10)\n",
        "        current_loss = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZ01ZVnrPTvM"
      },
      "source": [
        "Generate text using the trained net with different `temperature` parameter: `[0.1, 0.2, 0.5, 1.0, 2.0]`.\n",
        "\n",
        "Evaluate the results visually, try to interpret them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "collapsed": true,
        "id": "KW2Z3P0xPTvM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39fa1c09-5df1-425e-a6fa-b3f6da810ccc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "мой дядя самых честных правил своей.\n",
            "\n",
            "\n",
            "\n",
            "xix\n",
            "тау продили свобой,\n",
            "еду вы? мне имел охоты\n",
            "в молга евгению мносбуть,\n",
            "там белетых те на\n"
          ]
        }
      ],
      "source": [
        "# Text generation with different temperature values here\n",
        "def generate_text(model, temperature, seed_phrase, max_length=500):\n",
        "    model.eval()\n",
        "    hidden = model.init_hidden()\n",
        "    result = seed_phrase\n",
        "    input = chunk_to_tensor(seed_phrase).to(device)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        output, hidden = model(input[-1], hidden)\n",
        "        output_dist = output.view(-1).div(temperature).exp()\n",
        "        top_i = torch.multinomial(output_dist, 1)[0]\n",
        "\n",
        "        predicted_char = idx_to_token[top_i.item()]\n",
        "        result += predicted_char\n",
        "        input = chunk_to_tensor(result[-len(seed_phrase):]).to(device)\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "print(generate_text(lstm, seed_phrase='мой дядя самых честных правил ', max_length=100,temperature=0.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCLZ7LlUPTvM"
      },
      "source": [
        "### Saving and loading models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f34SpSuqPTvM"
      },
      "source": [
        "Save the model to the disk, then load it and generate text. Examples are available [here](https://pytorch.org/tutorials/beginner/saving_loading_models.html])."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "RujiQJ3NPTvM"
      },
      "outputs": [],
      "source": [
        "# Saving and loading code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Avq73D6PTvM"
      },
      "source": [
        "### References\n",
        "1. <a href='http://karpathy.github.io/2015/05/21/rnn-effectiveness/'> Andrew Karpathy blog post about RNN. </a>\n",
        "There are several examples of genration: Shakespeare texts, Latex formulas, Linux Sourse Code and children names.\n",
        "2. <a href='https://github.com/karpathy/char-rnn'> Repo with char-rnn code </a>\n",
        "3. Cool repo with PyTorch examples: [link](https://github.com/spro/practical-pytorch)"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}